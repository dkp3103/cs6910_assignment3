{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QAr9nbu_AUMC",
        "outputId": "84b687cb-da86-4434-9a50-102949955cb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            0                   1\n",
            "50000               repinundi           రేపినుండి\n",
            "50001               aavutundi            ఆవుతుంది\n",
            "50002            vrukshamtone          వృక్షంతోనే\n",
            "50003          mosukupogaladu        మోసుకుపోగలదు\n",
            "50004                seligman           సెలిగ్మన్\n",
            "...                       ...                 ...\n",
            "51195            telustondavi        తెలుస్తోందవి\n",
            "51196                patching             పేచింగ్\n",
            "51197  venakkiteesukoovaalane  వెనక్కితీసుకోవాలనే\n",
            "51198          roopaantaraalu          రూపాంతరాలు\n",
            "51199            chendindindi         చెందిందింది\n",
            "\n",
            "[1200 rows x 2 columns]\n",
            "English characters: ['o', 'c', 'r', 'u', 'y', 'h', 'x', 'e', 's', 'q', 'j', 'p', 'v', 'z', 'w', 'f', 'g', 'b', 't', 'm', 'l', 'd', 'n', 'a', 'k', 'i']\n",
            "Maximum length of an English word: 28\n",
            "Telugu characters: ['స', 'ు', 'ం', 'ై', 'ృ', 'జ', 'త', 'ఋ', 'ళ', 'ఠ', 'ఱ', 'ణ', 'డ', 'ప', 'ఇ', 'హ', 'అ', 'ీ', 'ఢ', 'ి', 'ూ', 'భ', 'వ', 'ఔ', 'ఛ', 'ఝ', 'ే', 'య', 'ఊ', 'ద', 'గ', 'మ', 'చ', 'ఐ', 'ఓ', 'శ', 'ఫ', 'ః', 'బ', 'ఎ', '్', 'ఏ', 'ఆ', 'ర', 'న', 'ఖ', 'ౌ', 'క', 'ల', 'ఈ', 'ధ', 'ఉ', 'ఒ', 'ట', 'ష', 'ఞ', 'ో', 'థ', 'ఘ', 'ా', 'ె', 'ొ']\n",
            "Maximum length of a Telugu word: 21\n",
            "Maximum length of a Telugu word in validation and test sets: 21\n",
            "Maximum length of an English word in validation and test sets: 28\n",
            "Number of English words: 51200\n",
            "Number of Telugu words: 51200\n",
            "Dimensions of English matrix: 51200, 30\n",
            "Dimensions of Telugu matrix: 51200, 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch =  1\n",
            "total loss =  tensor(0.4916, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Training Accuracy =  50.328125\n",
            "Validation accuracy =  46.09375\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    Input Target Predicted\n",
              "3169             bhatpara    పేడ      పెడా\n",
              "920                 vyaas    పేడ      పెడా\n",
              "2360              thamaku    పేడ      పెడా\n",
              "1738          viruddangaa    పేడ      పెడా\n",
              "1797          glaasullone    పేడ      పెడా\n",
              "1242             erickson    పేడ      పెడా\n",
              "3000         saagutondani    పేడ      పెడా\n",
              "787              vuntaayi    పేడ      పెడా\n",
              "2618       vaasudevaraavu    పేడ      పెడా\n",
              "2908                barai    పేడ      పెడా\n",
              "699           vidhvamsudu    పేడ      పెడా\n",
              "134                 manro    పేడ      పెడా\n",
              "701   sweekarinchinappudu    పేడ      పెడా\n",
              "2667                malda    పేడ      పెడా\n",
              "700                stetes    పేడ      పెడా\n",
              "3792              datacom    పేడ      పెడా\n",
              "780                tavvae    పేడ      పెడా\n",
              "578               anaheim    పేడ      పెడా\n",
              "3473                astra    పేడ      పెడా\n",
              "2680              aazaadh    పేడ      పెడా"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e773e538-1174-4767-a89d-e9385d655980\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input</th>\n",
              "      <th>Target</th>\n",
              "      <th>Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3169</th>\n",
              "      <td>bhatpara</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>920</th>\n",
              "      <td>vyaas</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2360</th>\n",
              "      <td>thamaku</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1738</th>\n",
              "      <td>viruddangaa</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1797</th>\n",
              "      <td>glaasullone</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1242</th>\n",
              "      <td>erickson</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3000</th>\n",
              "      <td>saagutondani</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>787</th>\n",
              "      <td>vuntaayi</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2618</th>\n",
              "      <td>vaasudevaraavu</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2908</th>\n",
              "      <td>barai</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699</th>\n",
              "      <td>vidhvamsudu</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>manro</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>701</th>\n",
              "      <td>sweekarinchinappudu</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2667</th>\n",
              "      <td>malda</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>700</th>\n",
              "      <td>stetes</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3792</th>\n",
              "      <td>datacom</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>780</th>\n",
              "      <td>tavvae</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>578</th>\n",
              "      <td>anaheim</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3473</th>\n",
              "      <td>astra</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2680</th>\n",
              "      <td>aazaadh</td>\n",
              "      <td>పేడ</td>\n",
              "      <td>పెడా</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e773e538-1174-4767-a89d-e9385d655980')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e773e538-1174-4767-a89d-e9385d655980 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e773e538-1174-4767-a89d-e9385d655980');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import torch\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login(key = \"b4f4cb721b8ca484302eddf5d87d6bb0be8b4f2c\")\n",
        "wandb.init(project=\"DL3_ASSN\")\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "path ='/content/tel_train.csv'\n",
        "df = pd.read_csv(path, header=None)\n",
        "english_words = df[0]\n",
        "Telugu_words = df[1]\n",
        "\n",
        "path_val = '/content/tel_valid.csv'\n",
        "df_val = pd.read_csv(path_val, header=None)\n",
        "english_words_val = df_val[0]\n",
        "Telugu_words_val = df_val[1]\n",
        "# Extract English and Telugu words from data frames\n",
        "\n",
        "path_test = '/content/tel_test.csv'\n",
        "df_test = pd.read_csv(path_test, header=None)\n",
        "english_words_test = df_test[0]\n",
        "Telugu_words_test = df_test[1]\n",
        "\n",
        "# Print rows from index 50000 onwards\n",
        "print(df.iloc[50000:])\n",
        "\n",
        "# Construct list of unique English characters and find maximum length of an English word\n",
        "english_char_list = []\n",
        "maxl_eng = -1\n",
        "eng_word_length=0\n",
        "for word in english_words:\n",
        "    maxl_eng = max(maxl_eng, len(word))\n",
        "    for char in word:\n",
        "        english_char_list.append(char)\n",
        "english_char_list = list(set(english_char_list))\n",
        "print(f\"English characters: {english_char_list}\")\n",
        "english_char_list.sort()\n",
        "print(f\"Maximum length of an English word: {maxl_eng}\")\n",
        "\n",
        "# Construct list of unique Telugu characters and find maximum length of a Telugu word\n",
        "Telugu_char_list = []\n",
        "maxl_tel = -1\n",
        "word_length=0\n",
        "for word in Telugu_words:\n",
        "    maxl_tel = max(maxl_tel, len(word))\n",
        "    for char in word:\n",
        "        Telugu_char_list.append(char)\n",
        "Telugu_char_list = list(set(Telugu_char_list))\n",
        "print(f\"Telugu characters: {Telugu_char_list}\")\n",
        "Telugu_char_list.sort()\n",
        "print(f\"Maximum length of a Telugu word: {maxl_tel}\")\n",
        "\n",
        "# Find maximum length of English and Telugu words in validation and test sets\n",
        "for word in Telugu_words_test:\n",
        "    maxl_tel = max(maxl_tel, len(word))\n",
        "for word in Telugu_words_val:\n",
        "    maxl_tel = max(maxl_tel, len(word))\n",
        "print(f\"Maximum length of a Telugu word in validation and test sets: {maxl_tel}\")\n",
        "for word in english_words_val:\n",
        "    maxl_eng = max(maxl_eng, len(word))\n",
        "for word in english_words_test:\n",
        "    maxl_eng = max(maxl_eng, len(word))\n",
        "print(f\"Maximum length of an English word in validation and test sets: {maxl_eng}\")\n",
        "predictions_vanilla_test = []\n",
        "\n",
        "# This function converts a word to a vector representation\n",
        "def word2vec(word, lang):\n",
        "    vec = []\n",
        "    count=0\n",
        "    if lang == \"english\":\n",
        "        # Add 1 to represent unknown characters\n",
        "        vec.append(len(english_char_list) + 1)\n",
        "        for char in word:\n",
        "            # Add 1 to represent character index (starting from 1)\n",
        "            if char in english_char_list:\n",
        "                vec.append(english_char_list.index(char) + 1)\n",
        "                count+=1\n",
        "            else:\n",
        "                vec.append(len(english_char_list) + 1)# Represent unknown characters\n",
        "                count+=1\n",
        "        # Padding with 0s to match max length\n",
        "        max_length=maxl_eng + 1\n",
        "        while len(vec) < max_length:\n",
        "            vec.append(0)\n",
        "            count+=1\n",
        "        # Add 0 at the end to represent end of word\n",
        "        vec.append(0)\n",
        "    else:\n",
        "        # Add 1 to represent unknown characters\n",
        "        vec.append(len(Telugu_char_list) + 1)\n",
        "        for char in word:\n",
        "            # Add 1 to represent character index (starting from 1)\n",
        "            if char in Telugu_char_list:\n",
        "                vec.append(Telugu_char_list.index(char) + 1)\n",
        "            else:\n",
        "                vec.append(len(Telugu_char_list) + 1) # Represent unknown characters\n",
        "                count+=1\n",
        "        # Padding with 0s to match max length\n",
        "        while len(vec) < maxl_tel + 1:\n",
        "            vec.append(0)\n",
        "            count+=1\n",
        "        # Add 0 at the end to represent end of word\n",
        "        vec.append(0)\n",
        "    return vec\n",
        "\n",
        "vec = word2vec(Telugu_words[10],\"Telugu\")\n",
        "\n",
        "def ip_matrix_construct(words, lang):\n",
        "    temp=[]\n",
        "    ans = []\n",
        "    for word in words:\n",
        "        count=0\n",
        "        embedding = word2vec(word, lang)\n",
        "        ans.append(embedding)\n",
        "    return ans\n",
        "\n",
        "# Construct input matrices for English and Telugu words\n",
        "english_matrix = ip_matrix_construct(english_words, \"english\")\n",
        "Telugu_matrix = ip_matrix_construct(Telugu_words, \"Telugu\")\n",
        "\n",
        "# Print the number of English and Telugu words\n",
        "print(f\"Number of English words: {len(english_matrix)}\")\n",
        "print(f\"Number of Telugu words: {len(Telugu_matrix)}\")\n",
        "\n",
        "# Convert input matrices to PyTorch tensors\n",
        "english_matrix = torch.tensor(english_matrix)\n",
        "Telugu_matrix = torch.tensor(Telugu_matrix)\n",
        "\n",
        "# Print the dimensions of the English and Telugu input matrices\n",
        "print(f\"Dimensions of English matrix: {len(english_matrix)}, {len(english_matrix[0])}\")\n",
        "print(f\"Dimensions of Telugu matrix: {len(Telugu_matrix)}, {len(Telugu_matrix[0])}\")\n",
        "\n",
        "# Construct input matrices for validation and test sets\n",
        "english_matrix_val = ip_matrix_construct(english_words_val, \"english\")\n",
        "english_matrix_val = torch.tensor(english_matrix_val)\n",
        "\n",
        "english_matrix_test = ip_matrix_construct(english_words_test, \"english\")\n",
        "english_matrix_test = torch.tensor(english_matrix_test)\n",
        "\n",
        "Telugu_matrix_val = ip_matrix_construct(Telugu_words_val, \"Telugu\")\n",
        "Telugu_matrix_val = torch.tensor(Telugu_matrix_val)\n",
        "\n",
        "Telugu_matrix_test =ip_matrix_construct(Telugu_words_test, \"Telugu\")\n",
        "Telugu_matrix_test = torch.tensor(Telugu_matrix_test)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,input_size, embedding_size, hidden_size, num_layers, p, cell_type, bidirectional,minimum):\n",
        "    encoded_output=[]\n",
        "    super(Encoder,self).__init__()\n",
        "    self.cell_type = cell_type\n",
        "    self.bidirectional = bidirectional\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.gru = nn.GRU(embedding_size, hidden_size, num_layers, dropout = p, bidirectional = bidirectional)\n",
        "    self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p, bidirectional = bidirectional)\n",
        "    self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout = p, bidirectional = bidirectional)\n",
        "    array_words=[]\n",
        "    \n",
        "  def forward(self, x):\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    if(self.cell_type == \"GRU\"):\n",
        "      output, hidden = self.gru(embedding)\n",
        "      return output, hidden\n",
        "    if(self.cell_type == \"RNN\"):\n",
        "      output, hidden = self.rnn(embedding)\n",
        "      return output, hidden\n",
        "    if(self.cell_type == \"LSTM\"):\n",
        "      outputs, (hidden,cell) = self.lstm(embedding)\n",
        "      return outputs,hidden, cell\n",
        "    \n",
        "\n",
        "  def initHidden(self):\n",
        "    temp=self.hidden_size\n",
        "    return torch.zeros(1, 1, temp, device=device)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p, cell_type,minimum):\n",
        "    decoded_output=[]\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.cell_type = cell_type\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    #self.minimum=minimum\n",
        "    self.fc = nn.Linear(hidden_size, output_size)  # fully connected.\n",
        "    self.gru = nn.GRU(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "    self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "    self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "    \n",
        "    #self.minimum=minimum\n",
        "  \n",
        "  def forward(self,x,output, hidden, cell = 0,enco_out=None):\n",
        "    temp,count=0\n",
        "    x = x.unsqueeze(0).int()\n",
        "    count+=1\n",
        "    embedding = self.dropout(self.embedding(x))\n",
        "    if(self.cell_type == \"GRU\"):\n",
        "        outputs, hidden = self.gru(embedding, hidden)\n",
        "        predictions = self.fc(outputs)\n",
        "        predictions = predictions.squeeze(0)\n",
        "        return predictions, hidden\n",
        "    elif(self.cell_type == \"RNN\"):\n",
        "        outputs, hidden = self.rnn(embedding, hidden)\n",
        "        predictions = self.fc(outputs)\n",
        "        predictions = predictions.squeeze(0)\n",
        "        return predictions, hidden\n",
        "    elif(self.cell_type == \"LSTM\"):\n",
        "        outputs, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
        "        count+=1\n",
        "        predictions = self.fc(outputs)\n",
        "        predictions = predictions.squeeze(0)\n",
        "        return predictions, hidden, cell\n",
        "\n",
        "  def initHidden(self):\n",
        "    temp=self.hidden_size\n",
        "    return torch.zeros(1, 1, temp, device=device)\n",
        "\n",
        "class Atten_decoder(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p, cell_type, bidirectional,minimum):\n",
        "    decoded_output=[]\n",
        "    super(Atten_decoder, self).__init__()\n",
        "    self.output_size = output_size\n",
        "    self.dropout = nn.Dropout(p)\n",
        "    self.cell_type = cell_type\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "    self.max_length = len(english_matrix[0])  \n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    #self.minimum=minimum\n",
        "\n",
        "    if(cell_type == \"GRU\"):\n",
        "      self.gru = nn.GRU(hidden_size, hidden_size, num_layers, dropout = p)\n",
        "    elif(cell_type == \"RNN\"):\n",
        "      self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, dropout = p)\n",
        "    elif(cell_type == \"LSTM\"):\n",
        "      self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, dropout = p)\n",
        "    self.attn = nn.Linear(hidden_size+embedding_size, self.max_length)\n",
        "    if bidirectional==False:\n",
        "        self.attn_combine = nn.Linear(hidden_size +embedding_size, hidden_size)\n",
        "    else:\n",
        "        self.attn_combine = nn.Linear(hidden_size*2 +embedding_size, hidden_size)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x,output, hidden, cell = 0,enco_out=None):\n",
        "    x = x.unsqueeze(0)\n",
        "    count=0\n",
        "    output=output.permute(1,0,2)\n",
        "    # embedded = self.embedding(x)\n",
        "    embedded = self.dropout(self.embedding(x))\n",
        "    attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "    attn_applied = torch.bmm(attn_weights.unsqueeze(1),output).squeeze(1)\n",
        "#     print(\"att applied done\")\n",
        "    output = torch.cat((embedded[0], attn_applied), 1)\n",
        "#     print(\"concat done\")\n",
        "    output = F.relu(self.attn_combine(output).unsqueeze(0))\n",
        "    if(self.cell_type == \"GRU\"):\n",
        "        # print(\"GRU\")\n",
        "        outputs, hidden = self.gru(output, hidden)\n",
        "        predictions = self.fc(outputs)\n",
        "        predictions = predictions.squeeze(0)\n",
        "        return predictions, hidden\n",
        "    elif(self.cell_type == \"RNN\"):\n",
        "        # print(\"GRU\")\n",
        "        outputs, hidden = self.rnn(output, hidden)\n",
        "        predictions = self.fc(outputs)\n",
        "        predictions = predictions.squeeze(0)\n",
        "        return predictions, hidden\n",
        "    elif(self.cell_type == \"LSTM\"):\n",
        "        # print(\"GRU\")\n",
        "        outputs, (hidden, cell) = self.lstm(output, (hidden, cell))\n",
        "        count+=1\n",
        "        predictions = self.fc(outputs)\n",
        "        predictions = predictions.squeeze(0)\n",
        "        return predictions, hidden, cell\n",
        "\n",
        "inp = []\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, cell_type, bidirectional, num_layers1,num_layers2,minimum):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        count=0\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.cell_type = cell_type\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_layers1 = num_layers1\n",
        "        self.num_layers2 = num_layers2\n",
        "        \n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        array_words2= []\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        temp_len=len(Telugu_char_list)\n",
        "        target_vocab_size = temp_len + 2  \n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "        count=0\n",
        "        if(self.cell_type == \"LSTM\"):\n",
        "            encoder_output, hidden, cell = self.encoder(source)\n",
        "            count+=1\n",
        "        else:\n",
        "            encoder_output, hidden = self.encoder(source)\n",
        "            count+=1\n",
        "        if(self.num_layers1 != self.num_layers2 or self.bidirectional == True):\n",
        "          temp1=0\n",
        "          temp = hidden[self.num_layers1 - 1] + hidden[self.num_layers1 - 1]\n",
        "          hidden=temp\n",
        "          temp2 = hidden.repeat(self.num_layers2,1,1)\n",
        "          hidden=temp2\n",
        "          if(self.cell_type == \"LSTM\"):\n",
        "              temp_2=0\n",
        "              c_temp = cell[self.num_layers1 - 1] + cell[self.num_layers1 - 1]\n",
        "              cell=c_temp\n",
        "              c_temp2 = cell.repeat(self.num_layers2,1,1)\n",
        "              cell=c_temp2\n",
        "        x = target[0]    \n",
        "        for t in range(0, target_len-1):\n",
        "            if(self.cell_type == \"LSTM\"):\n",
        "                hidden_temp=[]\n",
        "                output, hidden, cell = self.decoder(x, encoder_output,hidden, cell)\n",
        "                hidden_temp=hidden\n",
        "            else :\n",
        "                output, hidden = self.decoder(x,encoder_output, hidden)\n",
        "            outputs[t+1] = output\n",
        "            best_guess = output.argmax(1)\n",
        "            x = target[t+1] if random.random() < teacher_force_ratio else best_guess\n",
        "        return outputs\n",
        "\n",
        "def Accuracy(model, english_matrix, Telugu_matrix, epoch, batch_size):\n",
        "    count = 0\n",
        "    temp=len(english_matrix)\n",
        "    for batch_idx in range(1,(int)(temp/ batch_size)+1):\n",
        "        inp_data = english_matrix[batch_size * (batch_idx-1) : batch_size * ((batch_idx-1)+1)].to(device)\n",
        "        target = Telugu_matrix[batch_size * (batch_idx-1) : batch_size * ((batch_idx-1)+1)].to(device)\n",
        "        count+=1\n",
        "        output = model.forward(inp_data.T, target.T, 0)\n",
        "        output1=output.T\n",
        "        output = nn.Softmax(dim=2)(output)\n",
        "        output2= output.T\n",
        "        output = torch.argmax(output, dim=2)\n",
        "        output = output.T\n",
        "        for i in range(1,batch_size+1):\n",
        "            if(torch.equal(output[i-1][1:],target[i-1][1:])):\n",
        "                count += 1\n",
        "    return count * 100 / len(english_matrix)\n",
        "\n",
        "# creating list of expected marathi word and predicted marathi word.\n",
        "def word_prediction(model, english_matrix, telugu_matrix, batch_size):\n",
        "  inv=(int)(len(english_matrix) / batch_size)\n",
        "  for batch_idx in range(inv):\n",
        "        count=0\n",
        "        bs=batch_size\n",
        "        inp_data = english_matrix[bs * batch_idx : bs * (batch_idx+1)].to(device)\n",
        "        target = telugu_matrix[batch_size * batch_idx : batch_size * (batch_idx+1)].to(device)\n",
        "        output = model.forward(inp_data.T, target.T, 0)\n",
        "        prediction=[]\n",
        "        output = nn.Softmax(dim=2)(output)\n",
        "        output = torch.argmax(output, dim=2)\n",
        "        output = output.T\n",
        "        lp=len(inp_data)\n",
        "        for i in range(1,lp+1):\n",
        "          word = \"\"\n",
        "          count+=1\n",
        "          inp_word = inp_data[i-1]\n",
        "          for j in range(len(inp_word)):\n",
        "            eng_char_array_size=27\n",
        "            if(inp_word[j]>0 and inp_word[j]<eng_char_array_size):\n",
        "              temp_inp=english_char_list[inp_word[j]-1]\n",
        "              word += temp_inp\n",
        "          inp.append(word)\n",
        "        for i in range(len(target)):\n",
        "          count+=1\n",
        "          word1 = \"\"\n",
        "          word2 = \"\"\n",
        "          target_word = target[i]\n",
        "          output_word = output[i]\n",
        "          prediction.append(output_word)\n",
        "          for j in range(1,len(target_word)+1):\n",
        "            if(target_word[j-1]>0 and target_word[j-1]<63):\n",
        "              temp1 = Telugu_char_list[target_word[j-1] - 1]\n",
        "              word1 += temp1\n",
        "          count1=count\n",
        "          for j in range(1,len(output_word)+1):\n",
        "            if(output_word[j-1]>0 and output_word[j-1]<63):\n",
        "              temp2=Telugu_char_list[output_word[j-1] - 1]\n",
        "              word2 = word2 + temp2\n",
        "          temp = [word1, word2]        \n",
        "  df_test = pd.DataFrame({\"Input\": inp, \"Target\" : temp[0], \"Predicted\":temp[1]})\n",
        "  df_test.to_csv(\"Deepak_vanilla.csv\", index=False)\n",
        "\n",
        "def neural_network(cell_type, bidirectional, num_layers1,num_layers2, batch_size, embedding_size, hidden_size, num1_dropout,num2_dropout,attention):\n",
        "  num_epochs = 1\n",
        "  learning_rate = 0.001\n",
        "  minimum=0\n",
        "  count=0\n",
        "  eng_temp=len(english_char_list) + 2 \n",
        "  mar_temp=len(Telugu_char_list) + 2\n",
        "  input_size_encoder = eng_temp # intially= 30 || Now 26 + 1(start token) + 1 = 28 \n",
        "  input_size_decoder = mar_temp  # intitially = 66 || Now 63 + 1(start token) + 1 = 65\n",
        "  output_size        = mar_temp  # intitially = 66 || Now 63 + 1(start token) + 1 = 65\n",
        "\n",
        "  encoder_net = Encoder(input_size_encoder, embedding_size, hidden_size, num_layers1, num1_dropout, cell_type,bidirectional,minimum).to(device)\n",
        "  if(attention):\n",
        "    decoder_net = Atten_decoder(input_size_decoder,embedding_size,hidden_size,output_size,num_layers2,num2_dropout, cell_type, bidirectional,minimum).to(device)\n",
        "  else:\n",
        "    decoder_net = Decoder(input_size_decoder,embedding_size,hidden_size,output_size,num_layers2,num2_dropout, cell_type,minimum).to(device)\n",
        "\n",
        "  model = Seq2Seq(encoder_net, decoder_net, cell_type, bidirectional, num_layers1, num_layers2,minimum).to(device)\n",
        "  temp_l= len(Telugu_char_list) \n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  pad_idx = temp_l+1 # pading index for Telugu\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "  for epoch in range(1,num_epochs+1):\n",
        "      print(\"epoch = \",epoch)\n",
        "      model.train()\n",
        "      total_loss = 0\n",
        "      step = 0\n",
        "      val_loss=0\n",
        "      for batch_idx in range(1,((int)(len(english_matrix) / batch_size))+1):\n",
        "          inp_data = english_matrix[batch_size * (batch_idx-1) : batch_size * ((batch_idx-1)+1)].to(device)\n",
        "          target = Telugu_matrix[batch_size * (batch_idx-1) : batch_size * ((batch_idx-1)+1)].to(device)\n",
        "          target = target.T\n",
        "          val_loss=0\n",
        "          output = model(inp_data.T, target)\n",
        "          output = output[1:].reshape(-1, output.shape[2])\n",
        "          output_fnl=output[1:]\n",
        "          target = target[1:].reshape(-1)\n",
        "          optimizer.zero_grad()\n",
        "          loss = criterion(output, target)\n",
        "          val_loss+= loss/step\n",
        "          total_loss += loss\n",
        "          # Back prop\n",
        "          loss.backward()\n",
        "          val_loss=val_loss/100\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "          # Gradient descent step\n",
        "          optimizer.step()\n",
        "\n",
        "          step += 1\n",
        "          count += 1\n",
        "      \n",
        "      training_accuracy = Accuracy(model, english_matrix, Telugu_matrix, epoch, batch_size)\n",
        "      val_accuracy = Accuracy(model, english_matrix_val, Telugu_matrix_val, epoch, batch_size)\n",
        "      print(\"total loss = \",total_loss / step)\n",
        "      print(\"Training Accuracy = \", training_accuracy)\n",
        "      print(\"Validation accuracy = \",val_accuracy)\n",
        "  word_prediction(model, english_matrix_test, Telugu_matrix_test, batch_size,)\n",
        "  wandb.log({\"train_accuracy\": training_accuracy, \"validation_accuracy\": val_accuracy, \"training_loss\": total_loss / step,  'epoch': epoch})\n",
        "\n",
        "# neural_network(\"LSTM\", True, 1, 1, 64, 256, 512, 0.3, 0.3, True)\n",
        "\n",
        "df_test = pd.read_csv(\"Deepak_vanilla.csv\")\n",
        "df_test = df_test.sample(frac=1)\n",
        "df_test.head(20)\n",
        "sweep_config = {\n",
        "  \"name\": \"Assignment_3\",\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\": {\n",
        "      \"name\":\"validation_accuracy\",\n",
        "      \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"parameters\": {\n",
        "        \"num_layers\": {\n",
        "            \"values\": [1, 2, 3]\n",
        "        },\n",
        "        \"num_layers2\": {\n",
        "            \"values\": [1, 2, 3]\n",
        "        },\n",
        "        \"bidirectional\": {\n",
        "            \"values\": [True, False]\n",
        "        },\n",
        "\n",
        "        \"attention\": {\n",
        "            \"values\": [True, False]\n",
        "        },\n",
        "        \"hidden_size\": {\n",
        "            \"values\": [16,32,64,128, 256, 512]\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [32,64,128]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0.2,0.3,0.4,0.5]\n",
        "        },\n",
        "        \"num2_dropout\": {\n",
        "            \"values\": [0.2,0.3,0.4,0.5]\n",
        "        },\n",
        "        \"embedding_size\": {\n",
        "            \"values\": [16, 32, 64,128, 256, 512]\n",
        "        },\n",
        "        \n",
        "        \"cell_type\": {\n",
        "            \"values\": [\"GRU\", \"RNN\", \"LSTM\"]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, entity=\"deepak3103\", project=\"DL3_ASSN\")\n",
        "\n",
        "def Train():\n",
        "    # Initialize a new wandb run\n",
        "    wandb.init()\n",
        "    wandb.run.name = \"ct_\" + str(wandb.config.cell_type) +\"_bd_\" + str(wandb.config.bidirectional)  + \"_NL_\"+str(wandb.config.num_layers)+ \"_BS_\" + str(wandb.config.batch_size)+ \"_HS_\" + str(wandb.config.hidden_size) +\"_ES_\" + str(wandb.config.embedding_size)+ \"_DO_\" + str(wandb.config.num1_dropout)\n",
        "    print(wandb.run.name)\n",
        "    # Config is a variable that holds and saves hyperparameters and inputs\n",
        "    config = wandb.config\n",
        "    embedding_size = config.embedding_size\n",
        "    hidden_size = config.hidden_size\n",
        "    intial_epoch=0\n",
        "    cell_type = config.cell_type\n",
        "    num_layers1 = config.num_layers\n",
        "    num_layers2 = config.num_layers\n",
        "    batch_size = config.batch_size\n",
        "    num1_dropout = config.dropout\n",
        "    num2_dropout=config.dropout\n",
        "    bidirectional = config.bidirectional\n",
        "    attention=config.attention\n",
        "    intial_run=0\n",
        "\n",
        "    neural_network(cell_type, bidirectional, num_layers1, num_layers2, batch_size, embedding_size, hidden_size, num1_dropout, num2_dropout, attention)\n",
        "    wandb.run.save()\n",
        "\n",
        "wandb.agent(sweep_id, Train , count=100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WA-tzeLNAtBo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}